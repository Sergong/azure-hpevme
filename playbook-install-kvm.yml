---
- name: Install KVM, Virtualization Tools, and HPE VM Package
  hosts: azure_vms
  become: true
  gather_facts: true

  vars:
    kvm_packages:
      - qemu-kvm
      - qemu-block-extra
      - libvirt-daemon-system
      - libvirt-clients
      - virtinst
      - genisoimage
      - apparmor-utils
      - ceph-common
      - libvirt-daemon-driver-storage-rbd
      - nfs-common
      - gfs2-utils
      - corosync
      - dlm-controld
      - ceph
      - openvswitch-switch
      - pacemaker
      - pcs
      - resource-agents-extra
      - fence-agents-base
      - iptables-persistent
      - netfilter-persistent
    hpe_vm_deb_local: "hpe-vm_1.0.11-1_amd64.deb"
    hpe_vm_deb_remote: "/tmp/hpe-vm_1.0.11-1_amd64.deb"
    hpe_qcow_local: "hpe-vm-essentials-8.0.10-1.qcow2.gz"
    hpe_qcow_remote: "/tmp/hpe-vm-essentials-8.0.10-1.qcow2.gz"

  tasks:
    - name: Update apt cache
      ansible.builtin.apt:
        update_cache: true
        cache_valid_time: 3600

    # - name: Update All packages # noqa: package-latest
    #   ansible.builtin.apt:
    #     name: '*'
    #     state: latest
    #   notify: Reboot host

    - name: Enable password authentication in sshd_config
      ansible.builtin.lineinfile:
        path: /etc/ssh/sshd_config
        regexp: '^#?PasswordAuthentication\s+'
        line: 'PasswordAuthentication yes'
        state: present
        backup: true
      notify: Restart sshd

    - name: Ensure ChallengeResponseAuthentication is disabled (for clarity)
      ansible.builtin.lineinfile:
        path: /etc/ssh/sshd_config
        regexp: '^#?ChallengeResponseAuthentication\s+'
        line: 'ChallengeResponseAuthentication no'
        state: present
      notify: Restart sshd

    - name: Ensure UsePAM is enabled (required for password auth)
      ansible.builtin.lineinfile:
        path: /etc/ssh/sshd_config
        regexp: '^#?UsePAM\s+'
        line: 'UsePAM yes'
        state: present
      notify: Restart sshd

    - name: Force all notified handlers to run at this point
      ansible.builtin.meta: flush_handlers

    - name: Install KVM and virtualization packages
      ansible.builtin.apt:
        name: "{{ kvm_packages }}"
        state: present
      register: apt_install_result

    - name: Upload HPE VM deb package to remote host
      ansible.builtin.copy:
        src: "{{ hpe_vm_deb_local }}"
        dest: "{{ hpe_vm_deb_remote }}"
        mode: '0644'

    - name: Install HPE VM package
      ansible.builtin.apt:
        deb: "{{ hpe_vm_deb_remote }}"
        state: present

    - name: Ensure libvirt service is started and enabled
      ansible.builtin.systemd:
        name: libvirtd
        state: started
        enabled: true

    - name: Add admin user to libvirt group
      ansible.builtin.user:
        name: "{{ ansible_user }}"
        groups: libvirt
        append: true

    - name: Verify KVM installation
      ansible.builtin.command: kvm-ok
      register: kvm_ok_result
      failed_when: false
      changed_when: false

    - name: Display KVM verification result
      ansible.builtin.debug:
        msg: "{{ kvm_ok_result.stdout_lines }}"

    # Enable IP forwarding for overlay network
    - name: Enable IP forwarding
      ansible.posix.sysctl:
        name: net.ipv4.ip_forward
        value: '1'
        state: present
        sysctl_set: true
        reload: true

    - name: Ensure openvswitch-switch service is started and enabled
      ansible.builtin.systemd:
        name: openvswitch-switch
        state: started
        enabled: true

    - name: Create netplan config directory
      ansible.builtin.file:
        path: /etc/netplan/configs
        state: directory
        owner: root
        group: root
        mode: '0755'

    - name: Create hpe-vm compatible netplan config
      ansible.builtin.copy:
        dest: /etc/netplan/configs/netplan-hpe-vm-compat.yaml
        owner: root
        group: root
        mode: '0600'
        content: |
          network:
            version: 2
            ethernets:
              eth0:
                dhcp4: false
                addresses:
                  - {{ private_ip }}/24
                routes:
                  - to: default
                    via: 10.0.1.1
                  - to: 168.63.129.16
                    via: 10.0.1.1
                nameservers:
                  addresses:
                    - 168.63.129.16
                  search:
                    - hpevme.local
              eth1:
                dhcp4: no
                dhcp6: no
            bridges:
              mgmt:
                interfaces: [eth1]
                dhcp4: no
                dhcp6: no
          {% if gateway_host is defined and gateway_host %}
                addresses:
                  - {{ overlay_gateway_ip }}/24
          {% endif %}
                openvswitch: {}

    - name: Create full overlay netplan config
      ansible.builtin.copy:
        dest: /etc/netplan/configs/netplan-full-overlay.yaml
        owner: root
        group: root
        mode: '0600'
        content: |
          network:
            version: 2
            ethernets:
              eth0:
                dhcp4: false
                addresses:
                  - {{ private_ip }}/24
                routes:
                  - to: default
                    via: 10.0.1.1
                  - to: 168.63.129.16
                    via: 10.0.1.1
                nameservers:
                  addresses:
                    - 168.63.129.16
                  search:
                    - hpevme.local
              eth1:
                dhcp4: no
                dhcp6: no
            bridges:
              mgmt:
                interfaces: [eth1, vxlan0]
                dhcp4: no
                dhcp6: no
          {% if gateway_host is defined and gateway_host %}
                addresses:
                  - {{ overlay_gateway_ip }}/24
          {% endif %}
                openvswitch: {}
            tunnels:
              vxlan0:
                mode: vxlan
                id: 10
                remote: {{ remote_host_ip }}
                local: {{ private_ip }}
                port: 4789

    - name: Create network mode switching script
      ansible.builtin.copy:
        dest: /usr/local/bin/switch-network-mode.sh
        owner: root
        group: root
        mode: '0755'
        content: |
          #!/bin/bash
          set -e
          CONFIG_DIR="/etc/netplan/configs"
          ACTIVE_CONFIG="/etc/netplan/99-ovs.yaml"
          if [ "$1" == "hpe-vm" ]; then
              echo "Switching to hpe-vm compatible network configuration..."
              if sudo ovs-vsctl list-ports mgmt | grep -q '^vxlan0$'; then
                  echo "Removing vxlan0 from mgmt bridge..."
                  sudo ovs-vsctl --if-exists del-port mgmt vxlan0
              fi
              if ip link show vxlan0 >/dev/null 2>&1; then
                  echo "Deleting vxlan0 interface..."
                  sudo ip link delete vxlan0
              fi
              sudo cp "$CONFIG_DIR/netplan-hpe-vm-compat.yaml" "$ACTIVE_CONFIG"
              sudo netplan apply
              echo "Done. You can now run the hpe-vm utility."
          elif [ "$1" == "full" ]; then
              echo "Switching to full overlay network configuration..."
              sudo cp "$CONFIG_DIR/netplan-full-overlay.yaml" "$ACTIVE_CONFIG"
              sudo netplan apply
              echo "Done. The overlay network is now active."
          else
              echo "Usage: $0 [hpe-vm|full]"
              exit 1
          fi

    - name: Set network mode on gateway host (host 1) for hpe-vm compatibility
      ansible.builtin.command: /usr/local/bin/switch-network-mode.sh hpe-vm
      changed_when: false
      when: gateway_host is defined and gateway_host

    - name: Set network mode on non-gateway hosts to full overlay
      ansible.builtin.command: /usr/local/bin/switch-network-mode.sh full
      changed_when: false
      when: gateway_host is not defined or not gateway_host

    # Note: Overlay network routing (192.168.10.0/24 -> 10.0.1.4) is handled by Azure UDR
    # No manual routes needed on individual hosts - Azure handles this at the subnet level

    # Configure NAT for nested VM internet access (gateway host only)
    - name: Copy NAT setup script for nested VMs
      ansible.builtin.copy:
        src: setup-nested-vm-nat.sh
        dest: /tmp/setup-nested-vm-nat.sh
        mode: '0755'
      when: gateway_host is defined and gateway_host

    - name: Configure NAT for nested VM internet access (gateway host only)
      ansible.builtin.command: /tmp/setup-nested-vm-nat.sh
      when: gateway_host is defined and gateway_host
      register: nat_result
      changed_when: '"Added MASQUERADE" in nat_result.stdout'

    # Note: Using iptables-persistent instead of UFW to preserve libvirt rules
    # The NAT and routing scripts already configure necessary iptables rules
    - name: Save current iptables rules
      ansible.builtin.command: netfilter-persistent save
      changed_when: false

    # - name: Tasks for the first Host
    #   when: '"vme-kvm-vm1" in inventory_hostname'
    #   block:
    #     - name: Uploading HPE VM Essentials Disk to remote host
    #       ansible.posix.synchronize:
    #         src: "{{ hpe_qcow_local }}"
    #         dest: "{{ hpe_qcow_remote }}"
    #         partial: true
    #         perms: true

  handlers:
    - name: Restart sshd
      ansible.builtin.systemd:
        name: ssh
        state: restarted
        enabled: true

    - name: Reboot host
      ansible.builtin.reboot:
